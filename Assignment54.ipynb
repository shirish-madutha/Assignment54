{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe73e6-8a10-43b0-b809-a1bc6f64cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" To calculate the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use Bayes' theorem. Let:\n",
    "\n",
    "A = Employee is a smoker.\n",
    "B = Employee uses the health insurance plan.\n",
    "You are given:\n",
    "\n",
    "P(B) = 0.70 (probability that an employee uses the health insurance plan).\n",
    "P(A|B) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan).\n",
    "You want to find P(B|A), the probability that an employee uses the health insurance plan given that they are a smoker. You can use Bayes' theorem:\n",
    "\n",
    "P(B|A) = [P(A|B) * P(B)] / P(A)\n",
    "\n",
    "P(A) can be calculated using the law of total probability:\n",
    "\n",
    "P(A) = P(A|B) * P(B) + P(A|~B) * P(~B)\n",
    "\n",
    "Here, ~B represents an employee not using the health insurance plan.\n",
    "\n",
    "Assuming you have information about the percentage of non-health insurance users who are smokers (P(A|~B)), you can calculate P(A), and then use Bayes' theorem to find P(B|A). \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bbbfa-25de-4a91-b9e7-f906bbdf7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier used for different types of data:\n",
    "\n",
    "Bernoulli Naive Bayes is suitable for binary data, where features represent the presence (1) or absence (0) of specific attributes. It's commonly used for text classification problems, where each feature represents the presence or absence of a word in a document.\n",
    "\n",
    "Multinomial Naive Bayes is designed for discrete data, such as text data where features are integer counts, representing how many times each term appears in a document. It's used for tasks like text classification, spam detection, and sentiment analysis.\n",
    "\n",
    "The key difference lies in how they model the data and calculate conditional probabilities. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1254aeb-0cac-4f67-9718-25aed25972a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q3. How does Bernoulli Naive Bayes handle missing values? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\"  Bernoulli Naive Bayes typically handles missing values by treating them as if the feature is absent. In the context of text data, if a term is missing in a document, it's treated as if it doesn't appear in that document (i.e., a 0 in the binary feature vector). This is a simplification, and whether this approach is suitable depends on the specific problem and dataset. You can also use techniques to impute missing values if needed, but this may complicate the Naive Bayes model.\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ab0f0-0b20-4b48-a1ca-b6759e1a1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4. Can Gaussian Naive Bayes be used for multi-class classification? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Gaussian Naive Bayes is primarily used for binary and continuous data, so it's not typically used for multi-class classification out of the box. However, you can adapt it for multi-class problems by using techniques like one-vs-all (OvA) or one-vs-one (OvO) strategies. In OvA, you train multiple binary classifiers, each distinguishing one class from the rest, and then combine their outputs to make a multi-class prediction. In OvO, you train a binary classifier for every pair of classes. Gaussian Naive Bayes can be used as one of these binary classifiers. These strategies allow you to extend Gaussian Naive Bayes for multi-class classification tasks. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3e76d-dd11-4546-bb87-9cc724c77a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4037517d-eb97-4883-93ba-210ca5e4324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data.csv\", header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f6ce1cc-bc87-43b0-851c-e4706f74a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8905299739357081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       694\n",
      "           1       0.89      0.83      0.86       457\n",
      "\n",
      "    accuracy                           0.89      1151\n",
      "   macro avg       0.89      0.88      0.88      1151\n",
      "weighted avg       0.89      0.89      0.89      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred=bnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32012d1-eacc-4c31-8f9f-ec00aa0215e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8132059079061685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       694\n",
      "           1       0.77      0.76      0.76       457\n",
      "\n",
      "    accuracy                           0.81      1151\n",
      "   macro avg       0.80      0.80      0.80      1151\n",
      "weighted avg       0.81      0.81      0.81      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred=mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c75f14-bcf8-4a96-98d5-62c303778924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8253692441355344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.84       694\n",
      "           1       0.71      0.96      0.81       457\n",
      "\n",
      "    accuracy                           0.83      1151\n",
      "   macro avg       0.84      0.85      0.82      1151\n",
      "weighted avg       0.86      0.83      0.83      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred=gnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
